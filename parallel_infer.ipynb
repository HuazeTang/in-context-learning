{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9f9fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/xi/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 05:14:54 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-10 05:15:04 [config.py:3392] Your device 'Tesla V100-PCIE-32GB' (with compute capability 7.0) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "WARNING 08-10 05:15:04 [config.py:3443] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-10 05:15:04 [config.py:1604] Using max model len 4096\n",
      "WARNING 08-10 05:15:04 [arg_utils.py:1690] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 05:15:04,807\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 05:15:04 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/root/autodl-tmp/data/model_ckpts/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B', speculative_config=None, tokenizer='/root/autodl-tmp/data/model_ckpts/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/autodl-tmp/data/model_ckpts/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 05:15:07 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-10 05:15:07 [cuda.py:395] Using XFormers backend.\n",
      "INFO 08-10 05:15:08 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-10 05:15:08 [model_runner.py:1083] Starting to load model /root/autodl-tmp/data/model_ckpts/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.23s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.20s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.21s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 05:15:17 [default_loader.py:262] Loading weights took 8.79 seconds\n",
      "INFO 08-10 05:15:18 [model_runner.py:1115] Model loading took 15.2898 GiB and 9.063735 seconds\n",
      "INFO 08-10 05:15:19 [worker.py:295] Memory profiling takes 1.20 seconds\n",
      "INFO 08-10 05:15:19 [worker.py:295] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB\n",
      "INFO 08-10 05:15:19 [worker.py:295] model weights take 15.29GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 11.77GiB.\n",
      "INFO 08-10 05:15:19 [executor_base.py:113] # cuda blocks: 5357, # CPU blocks: 1820\n",
      "INFO 08-10 05:15:19 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 20.93x\n",
      "INFO 08-10 05:15:23 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:23<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 05:15:46 [model_runner.py:1537] Graph capturing finished in 23 secs, took 1.29 GiB\n",
      "INFO 08-10 05:15:46 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 28.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 368.24it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.30s/it, est. speed input: 1.21 toks/s, output: 38.76 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¼Œç„¶åŽè¯´è¯´ä½ èƒ½åšä»€ä¹ˆï¼Ÿ\n",
      "\n",
      "ä½ å¥½ï¼ðŸ‘‹ æˆ‘æ˜¯DeepSeek-R1ï¼Œä¸€æ¬¾ç”±æ·±åº¦æ±‚ç´¢å…¬å¸å¼€å‘çš„å¤§è¯­è¨€æ¨¡åž‹åŠ©æ‰‹ã€‚ä½ å¯ä»¥æŠŠæˆ‘å½“ä½œä¸€ä¸ªçŸ¥è¯†ä¸°å¯Œã€ä¹äºŽåŠ©äººçš„æ­æ¡£ï¼Œéšæ—¶å‡†å¤‡å›žç­”ä½ çš„é—®é¢˜ã€æä¾›å¸®åŠ©æˆ–é™ªä½ èŠå¤©ã€‚\n",
      "\n",
      "æˆ‘èƒ½åšçš„äº‹æƒ…æœ‰å¾ˆå¤šï¼Œæ¯”å¦‚ï¼š\n",
      "\n",
      "ðŸŽ“ å­¦ä¹ å’Œæ•™è‚²ï¼šè§£ç­”åŠŸè¯¾é—®é¢˜ã€è§£é‡Šå¤æ‚çš„æ¦‚å¿µï¼ˆæ¯”å¦‚æ•°å­¦ã€ç‰©ç†ã€åŽ†å²ç­‰ï¼‰ï¼Œå¸®ä½ å†™ä½œæ–‡ã€æ¶¦è‰²è‹±æ–‡ï¼Œç”šè‡³å‡†å¤‡è€ƒè¯•ã€‚\n",
      "\n",
      "ðŸ’¼ å·¥ä½œåŠ©ç†ï¼šå¸®ä½ å†™é‚®ä»¶ã€æ•´ç†æ–‡æ¡£ã€åˆ†æžæ•°æ®ã€ç”ŸæˆæŠ¥å‘Šã€åˆ¶ä½œPPTå¤§çº²ç­‰ã€‚ä¸Šä¼ Wordã€Excelã€PDFç­‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/root/autodl-tmp/data/model_ckpts/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "    max_model_len=4096,\n",
    "    gpu_memory_utilization=0.9\n",
    ")\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=128)\n",
    "prompts = [\"ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\"]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
