_target_: src.models.qwen_vllm_model.QwenVLLMModel
config:
  type: qwen
  model_path: "/root/autodl-tmp/hf_cache/Qwen/Qwen3-14B"
  tokenizer_path: "/root/autodl-tmp/hf_cache/Qwen/Qwen3-14B"
  torch_dtype: bfloat16
  device_map: "cuda:0"
  enable_thinking: false  # 控制是否启用思考模式
  terminators:
    - "</s>"
    - "<|endoftext|>"
    - "<|endoftext|>"
    - "<|im_end|>"
  generation_params:
    max_new_tokens: 256
    temperature: 0.6
    top_p: 0.95
    top_k: 20
    do_sample: true
    eos_token_id: 151645
    output_hidden_states: false
    return_dict_in_generate: false
  layer_num: 36