{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3f29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/xi/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 15:57:52 [__init__.py:235] Automatically detected platform cuda.\n",
      "WARNING 08-08 15:58:03 [config.py:3392] Your device 'Tesla V100-PCIE-32GB' (with compute capability 7.0) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "WARNING 08-08 15:58:03 [config.py:3443] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-08 15:58:03 [config.py:1604] Using max model len 40960\n",
      "WARNING 08-08 15:58:03 [arg_utils.py:1690] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 08-08 15:58:03 [arg_utils.py:1486] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 15:58:03,784\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 15:58:03 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 08-08 15:58:03 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/root/autodl-tmp/data/model_ckpts/Qwen/Qwen3-8B', speculative_config=None, tokenizer='/root/autodl-tmp/data/model_ckpts/Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/autodl-tmp/data/model_ckpts/Qwen/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 08-08 15:58:06 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-08 15:58:06 [cuda.py:395] Using XFormers backend.\n",
      "INFO 08-08 15:58:07 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-08 15:58:07 [model_runner.py:1083] Starting to load model /root/autodl-tmp/data/model_ckpts/Qwen/Qwen3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:20<01:22, 20.52s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:41<01:03, 21.08s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:03<00:42, 21.15s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:18<00:18, 18.95s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:25<00:00, 14.66s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:25<00:00, 17.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 15:59:33 [default_loader.py:262] Loading weights took 85.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 15:59:34 [model_runner.py:1115] Model loading took 15.2683 GiB and 86.185500 seconds\n",
      "INFO 08-08 15:59:35 [worker.py:295] Memory profiling takes 0.99 seconds\n",
      "INFO 08-08 15:59:35 [worker.py:295] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB\n",
      "INFO 08-08 15:59:35 [worker.py:295] model weights take 15.27GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 11.81GiB.\n",
      "INFO 08-08 15:59:36 [executor_base.py:113] # cuda blocks: 5374, # CPU blocks: 1820\n",
      "INFO 08-08 15:59:36 [executor_base.py:118] Maximum concurrency for 40960 tokens per request: 2.10x\n",
      "INFO 08-08 15:59:39 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:24<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 16:00:04 [model_runner.py:1537] Graph capturing finished in 25 secs, took 1.29 GiB\n",
      "INFO 08-08 16:00:04 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 30.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LLM' object has no attribute 'get_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/autodl-tmp/data/model_ckpts/Qwen/Qwen3-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m获取这段文本的embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LLM' object has no attribute 'get_embeddings'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"/root/autodl-tmp/data/model_ckpts/Qwen/Qwen3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "492ec932",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Question: In a population of giraffes, an environmental change occurs that favors individuals that are tallest. As a result, more of the taller individuals are able to obtain nutrients and survive to pass along their genetic information. This is an example of\\nA. directional selection.\\nB. stabilizing selection.\\nC. sexual selection.\\nD. disruptive selection.\\n\\nAnswer: \"\n",
    "qa = [f\"{question}_{answer}.\" for answer in [\"A\", \"B\", \"C\", \"D\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ebf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.engine.arg_utils import EngineArgs\n",
    "from vllm.engine.llm_engine import LLMEngine\n",
    "import torch\n",
    "\n",
    "# 直接使用LLMEngine而非高级LLM接口\n",
    "engine_args = EngineArgs(model=\"/root/autodl-tmp/data/model_ckpts/Qwen/Qwen3-8B\")\n",
    "engine = LLMEngine.from_engine_args(engine_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e8d9361",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UniProcExecutor' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(all_embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# 使用高效的批处理方式获取所有嵌入\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     all_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings_efficiently\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36mget_embeddings_efficiently\u001b[0;34m(texts, batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(padded_ids)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 直接访问embedding层，避免全部前向传播\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m embedding_layer \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mget_input_embeddings()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     27\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m embedding_layer(input_ids)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UniProcExecutor' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "def get_pad_token_id(engine):\n",
    "    # 方法1: 从底层tokenizer获取\n",
    "    if hasattr(engine.tokenizer, \"tokenizer\"):  # 访问实际的HF tokenizer\n",
    "        if hasattr(engine.tokenizer.tokenizer, \"pad_token_id\"):\n",
    "            return engine.tokenizer.tokenizer.pad_token_id\n",
    "    \n",
    "    # 方法2: 使用通用默认值\n",
    "    return 0  # 很多模型使用0作为padding token\n",
    "\n",
    "def get_embeddings_efficiently(texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # 获取tokenized输入\n",
    "        token_ids_list = [engine.tokenizer.encode(text) for text in batch]\n",
    "        max_len = max(len(ids) for ids in token_ids_list)\n",
    "        \n",
    "        # 填充到相同长度\n",
    "        padded_ids = [ids + [engine.tokenizer.tokenizer.pad_token_id] * (max_len - len(ids)) for ids in token_ids_list]\n",
    "        input_ids = torch.tensor(padded_ids).cuda()\n",
    "        \n",
    "        # 直接访问embedding层，避免全部前向传播\n",
    "        embedding_layer = engine.model_executor.model.get_input_embeddings()\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = embedding_layer(input_ids)\n",
    "        \n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "for _ in range(40):\n",
    "    # 使用高效的批处理方式获取所有嵌入\n",
    "    all_embeddings = get_embeddings_efficiently(qa, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b210c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"/root/autodl-tmp/data/model_ckpts/Qwen/Qwen3-8B\", task=\"embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9ad74f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Embedding API is not supported by this model. Please set `--task embed`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xi/lib/python3.9/site-packages/vllm/entrypoints/llm.py:1175\u001b[0m, in \u001b[0;36mLLM.embed\u001b[0;34m(self, prompts, truncate_prompt_tokens, use_tqdm, pooling_params, lora_request)\u001b[0m\n\u001b[1;32m   1173\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mmodel_config\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39msupported_tasks:\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding API is not supported by this model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set `--task embed`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1178\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m   1179\u001b[0m     prompts,\n\u001b[1;32m   1180\u001b[0m     truncate_prompt_tokens\u001b[38;5;241m=\u001b[39mtruncate_prompt_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     pooling_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1185\u001b[0m )\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [EmbeddingRequestOutput\u001b[38;5;241m.\u001b[39mfrom_base(item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items]\n",
      "\u001b[0;31mValueError\u001b[0m: Embedding API is not supported by this model. Please set `--task embed`."
     ]
    }
   ],
   "source": [
    "llm.embed(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd4e28f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UniProcExecutor' object has no attribute 'worker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         handle\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m     25\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m这是一个测试文本\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mget_last_layer_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 4. 查看结果\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHidden state shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_state\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mget_last_layer_output\u001b[0;34m(text, llm)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_last_layer_output\u001b[39m(text, llm):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# 获取内部模型\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworker\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# 注册一个钩子来捕获最后一层输出\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UniProcExecutor' object has no attribute 'worker'"
     ]
    }
   ],
   "source": [
    "def get_last_layer_output(text, llm):\n",
    "    # 获取内部模型\n",
    "    model = llm.llm_engine.model_executor.worker.model\n",
    "    \n",
    "    # 注册一个钩子来捕获最后一层输出\n",
    "    last_hidden_states = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        last_hidden_states.append(output[0].detach().cpu())\n",
    "    \n",
    "    # 为最后一层注册钩子\n",
    "    # Llama和大多数模型的最后一层通常在这个位置\n",
    "    handle = model.layers[-1].register_forward_hook(hook)\n",
    "    \n",
    "    try:\n",
    "        # 简单运行一个生成，触发模型前向传播\n",
    "        outputs = llm.generate([text], max_tokens=1)\n",
    "        \n",
    "        # 钩子已经捕获了最后一层的输出\n",
    "        return last_hidden_states[0]\n",
    "    finally:\n",
    "        # 删除钩子，防止内存泄漏\n",
    "        handle.remove()\n",
    "\n",
    "text = \"这是一个测试文本\"\n",
    "hidden_state = get_last_layer_output(text, llm)\n",
    "\n",
    "# 4. 查看结果\n",
    "print(f\"Hidden state shape: {hidden_state.shape}\")\n",
    "print(f\"Sample values: {hidden_state[0, :5]}\")  # 打印前几个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80335e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM结构:\n",
      "- DEPRECATE_LEGACY\n",
      "- apply_model\n",
      "- beam_search\n",
      "- chat\n",
      "- classify\n",
      "- collective_rpc\n",
      "- default_sampling_params\n",
      "- deprecate_legacy_api\n",
      "- embed\n",
      "- encode\n",
      "- engine_class\n",
      "- generate\n",
      "- get_default_sampling_params\n",
      "- get_metrics\n",
      "- get_tokenizer\n",
      "- llm_engine\n",
      "- request_counter\n",
      "- reset_prefix_cache\n",
      "- score\n",
      "- set_tokenizer\n",
      "- sleep\n",
      "- start_profile\n",
      "- stop_profile\n",
      "- wake_up\n",
      "\n",
      "LLM Engine结构:\n",
      "- DO_VALIDATE_OUTPUT\n",
      "- abort_request\n",
      "- add_logger\n",
      "- add_lora\n",
      "- add_request\n",
      "- async_callbacks\n",
      "- cache_config\n",
      "- cached_scheduler_outputs\n",
      "- check_health\n",
      "- collective_rpc\n",
      "- create_trace_span\n",
      "- decoding_config\n",
      "- detokenizer\n",
      "- device_config\n",
      "- do_log_stats\n",
      "- do_tracing\n",
      "- enable_output_validation\n",
      "- from_engine_args\n",
      "- from_vllm_config\n",
      "- generation_config_fields\n",
      "- get_decoding_config\n",
      "- get_lora_config\n",
      "- get_model_config\n",
      "- get_num_unfinished_requests\n",
      "- get_parallel_config\n",
      "- get_scheduler_config\n",
      "- get_tokenizer\n",
      "- get_tokenizer_group\n",
      "- get_vllm_config\n",
      "- has_unfinished_requests\n",
      "- has_unfinished_requests_for_virtual_engine\n",
      "- input_preprocessor\n",
      "- is_sleeping\n",
      "- is_tracing_enabled\n",
      "- list_loras\n",
      "- load_config\n",
      "- log_stats\n",
      "- lora_config\n",
      "- model_config\n",
      "- model_executor\n",
      "- observability_config\n",
      "- output_processor\n",
      "- parallel_config\n",
      "- pin_lora\n",
      "- process_request_outputs_callback\n",
      "- remove_logger\n",
      "- remove_lora\n",
      "- reset_mm_cache\n",
      "- reset_prefix_cache\n",
      "- scheduler\n",
      "- scheduler_config\n",
      "- scheduler_contexts\n",
      "- seq_counter\n",
      "- seq_id_to_seq_group\n",
      "- sleep\n",
      "- speculative_config\n",
      "- start_profile\n",
      "- step\n",
      "- stop_profile\n",
      "- stop_remote_worker_execution_loop\n",
      "- tokenizer\n",
      "- tracer\n",
      "- use_cached_outputs\n",
      "- validate_output\n",
      "- validate_outputs\n",
      "- vllm_config\n",
      "- wake_up\n",
      "\n",
      "Model Executor结构:\n",
      "- add_lora\n",
      "- apply_model\n",
      "- cache_config\n",
      "- check_health\n",
      "- check_health_async\n",
      "- collective_rpc\n",
      "- determine_num_available_blocks\n",
      "- device_config\n",
      "- driver_worker\n",
      "- execute_model\n",
      "- execute_model_async\n",
      "- initialize_cache\n",
      "- is_sleeping\n",
      "- list_loras\n",
      "- load_config\n",
      "- lora_config\n",
      "- model_config\n",
      "- observability_config\n",
      "- parallel_config\n",
      "- pin_lora\n",
      "- reinitialize_distributed\n",
      "- remove_lora\n",
      "- save_sharded_state\n",
      "- scheduler_config\n",
      "- shutdown\n",
      "- sleep\n",
      "- sleeping_tags\n",
      "- speculative_config\n",
      "- start_profile\n",
      "- stop_profile\n",
      "- stop_remote_worker_execution_loop\n",
      "- stop_remote_worker_execution_loop_async\n",
      "- supported_pooling_tasks\n",
      "- uses_ray\n",
      "- vllm_config\n",
      "- wake_up\n"
     ]
    }
   ],
   "source": [
    "print(\"LLM结构:\")\n",
    "for attr in dir(llm):\n",
    "    if not attr.startswith('_'):\n",
    "        print(f\"- {attr}\")\n",
    "\n",
    "print(\"\\nLLM Engine结构:\")\n",
    "for attr in dir(llm.llm_engine):\n",
    "    if not attr.startswith('_'):\n",
    "        print(f\"- {attr}\")\n",
    "\n",
    "print(\"\\nModel Executor结构:\")\n",
    "for attr in dir(llm.llm_engine.model_executor):\n",
    "    if not attr.startswith('_'):\n",
    "        print(f\"- {attr}\")\n",
    "\n",
    "# 如果是多进程执行器，可能需要检查worker\n",
    "if hasattr(llm.llm_engine.model_executor, \"worker\"):\n",
    "    print(\"\\nWorker结构:\")\n",
    "    for attr in dir(llm.llm_engine.model_executor.worker):\n",
    "        if not attr.startswith('_'):\n",
    "            print(f\"- {attr}\")\n",
    "\n",
    "# 对于最新版本可能是driver\n",
    "if hasattr(llm.llm_engine.model_executor, \"driver\"):\n",
    "    print(\"\\nDriver结构:\")\n",
    "    for attr in dir(llm.llm_engine.model_executor.driver):\n",
    "        if not attr.startswith('_'):\n",
    "            print(f\"- {attr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
